# Dataset Reproduction 
For an extended documentation of the dataset creation process, please refer to our [Huggingface page](https://huggingface.co/datasets/LaughingLogits/Stackless_Java_V2).

All the packages needed to run the scripts below can be found either in the [requirements.txt](./requirements.txt) file for the Python code, and in the [package.json](./package.json) file for the JavaScript code.

To run the Python scripts, make sure that [Python (3.10-3.11) and pip](https://www.python.org/downloads/) are installed on your device. Then, create a virtual environment and activate it. After that, you can install the requirements using ```pip install -r requirements.txt```. 

To run the Js script, make sure that [Node.js and npm](https://nodejs.org/en) are intalled on your device. Then, you can install the requirements using ```npm install```. After that, you can run the script using ```node repository_scraper.js “Lang1” “Lang 2” … “Lang n” AmountOfRepos```. For example: ```node repository_scraper.js “Java”, “Python” 3500```. This will return 2 JSON files, one with 3500 Java repos, and another one with 3500 Python repos. Use spaces between languages and capital letters for each of them. You have to input at least one language.    
 

**The first 3 steps are optional.** 

1. Start by scraping GitHub repositories using the [*repository_scraper.js*](./repository_scraper.js) script. You will need to set up your GitHub account token and make sure that [Octokit](https://www.npmjs.com/package/@octokit/rest) is installed. You can customize the scraping to match your needs by manually adjusting the search filters (license type, repository creation date, repository main language, repository star count), the limit reset timeout duration, what fields to extract for each repository, or the total number of repositoies to scrape. Save the json file to your desired path. 

2. Use the [*repository_deduplication.py*](./repository_deduplication.py) script to remove any potential repository duplicates (obtained due to pagination) from the json file. Make sure to place the correct path to the json file from the previous step. The script was created to remove duplicates for multiple languages. In case you have only one json file (e.g. Java repos only), you can just pass an array with its path directly to the *remove_and_check_duplicates* method.

3. Extract all *.java* files from each repository in the json file using the [*file_extraction.py*](./file_extraction.py) script. For that, you will need to set up your Huggingface token, and the location to where you want to save intermediate results. The script is harcoded for the Java language, however, you can change it to any desired language, as long as it matches the main language of your scraped repositories. If you are using your own list of scraped repositories, ensure that the path to your file is properly configured. Otherwise, you can use the list we provide, which is found in the [*JavaStrongCopyLeft10500.json*](./JavaStrongCopyLeft10500.json) file. You can also manually change the file filtering options, such as the file size, the minimum amount of words per file, or even the fileds extracted for each entry in the dataset. Furthermore, in case you are using this script for other language than Java, make sure that the extension you are looking for can be found in the [*langs_extension.json*](./langs_extension.json) file which was also used for [The Stack v1](https://gist.github.com/ppisarczyk/43962d06686722d26d176fad46879d41). Lastly, save the dataset to your Huggingface location. 

4. Starting from this step, you can either use the dataset you created, or our [Raw_Java](https://huggingface.co/datasets/LaughingLogits/Stackless_Java_V2) dataset that we provide. This step removes exact duplicates between our custom dataset and Java-Stack v2, but also within our custom dataset itself, using the [*exact_deduplication.py*](./exact_deduplication.py) script. You will need to set up your Huggingface token, the path to the Java-Stack v2, either from the disk, or directly from Huggingface, and the path to your own dataset if you are not using Raw_Java. Make sure to save the exact-deduplicated dataset to your Huggingface location. 

5. This represents the first step in the near-deduplication process, namely the [*lsh_creation.py*](./lsh_creation.py) script. Start by creating the MinHashLSH object using the [datasketch](https://ekzhu.com/datasketch/lsh.html) library. For that, you will need to set up your Huggingface token and load the exact-deduplicated dataset from the previous step. For creating the MinHashes of the files, we follow a similar approach to the one introduced in the [Mining of Massive Datasets](https://www.cambridge.org/core/books/mining-of-massive-datasets/C1B37BA2CBB8361B94FDD1C6F4E47922) book.  The MinHashLSH object stores the MinHashes of all the files in our custom dataset. This is more space efficient rather than storing all the MinHashes for the Java-Stack v2, which contains over 220 M files. We also use a Jaccard Similarity threshold of 0.7, which was used in the case of both [SantaCoder](https://arxiv.org/abs/2301.03988) and [StarCoder](https://arxiv.org/abs/2305.06161) models. You can also adjust the number of permutations used, shingle size, hash function, precision-recall weights, or even the similarity threshold.  Lastly, save the MinHashLSH pickle object to your location. 

6. This step uses the [*near_deduplication.py*](./near_deduplication.py) script. You will need to set up your Huggingface token and the path to the MinHashLSH object from the previous step. The object is loaded globally in order to share it across multiprocesses and to avoid the creation of copies for each process, which can lead to memory issues. You will also need to set the path to the Java-Stack v2. The script iterates through the Stack files and queries the MinHashLSH object based on their MinHashes. For each file in the Java-Stack v2, the script returns an array with the IDs of near-duplicate files from our custom dataset.  Lastly, save the updated Java-Stack v2 dataset to your location. 

7. This step uses the [*convert_ids.py*](./convert_ids.py) script which converts the list of near-duplicate IDs from our dataset into a list of near-duplicate IDs from Java-Stack v2 with respect to each file in our dataset. You will need to set up your Huggingface token and configure the paths to our custom exact-deduplicated dataset and the updated Java-Stack v2 from the previous step. After conversion, the IDs are mapped into a new column that is added to our exact-deduplicated dataset. This column contains an empty array if there are no near-duplicate files from Java-Stack v2 with respect to the current file in our dataset. Otherwise, there is an array of integers, representing the IDs of near-duplicate files from Java-Stack v2 with respect to the current file in our dataset. Thus, the near-duplicates are only flagged in our final dataset, but you can simply remove them by filtering by the array size in the *near_dups_stkv2_idx* column. Lastly, save the near-deduplicated dataset to your Huggingface path. 

 8. The last step involves the removal of autogenerated Java files from our dataset, using the [*autogenerated_files.py*](./autogenerated_files.py) script. For that, you will need to set up your Huggingface token, and the path to the near-deduplicated dataset from the previous step. The keywords used in finding autogenerated Java files were derived from the [Stack v2](https://arxiv.org/abs/2402.19173) approach and manual file inspection. You can adjust this keyword list to your needs. Finally, save the final dataset to your Huggingface path.  
